{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import altair as alt\n",
    "import sklearn.metrics as metrics\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "from imblearn.under_sampling import CondensedNearestNeighbour, RandomUnderSampler, EditedNearestNeighbours, RepeatedEditedNearestNeighbours\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE, SVMSMOTE\n",
    "from imblearn.combine import SMOTETomek, SMOTEENN\n",
    "\n",
    "df = pd.read_csv('../data/processed/balanced_data.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    200\n",
       "2    200\n",
       "5    181\n",
       "4    150\n",
       "1    150\n",
       "0    150\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.target.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>info</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>1623</td>\n",
       "      <td>King JOhn</td>\n",
       "      <td>es that i will and wherefore will i do it i to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>1528</td>\n",
       "      <td>The book of the Courtier</td>\n",
       "      <td>then the soul freed from vice purged by studie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>1611</td>\n",
       "      <td>the tempest</td>\n",
       "      <td>why as i told thee tis a custom with him i th ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>1550</td>\n",
       "      <td>Great sonnets</td>\n",
       "      <td>t me not to the marriage of true minds admit i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>1600</td>\n",
       "      <td>A midsummer nights drea</td>\n",
       "      <td>overs and madmen have such seething brains suc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     date                      info  \\\n",
       "235  1623                 King JOhn   \n",
       "242  1528  The book of the Courtier   \n",
       "238  1611               the tempest   \n",
       "180  1550             Great sonnets   \n",
       "197  1600   A midsummer nights drea   \n",
       "\n",
       "                                                  text  \n",
       "235  es that i will and wherefore will i do it i to...  \n",
       "242  then the soul freed from vice purged by studie...  \n",
       "238  why as i told thee tis a custom with him i th ...  \n",
       "180  t me not to the marriage of true minds admit i...  \n",
       "197  overs and madmen have such seething brains suc...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_list = [\n",
    "    [0, 1670, 1800, 1870, 1910, 1945, np.inf],\n",
    "    [0, 1670, 1830, 1870, 1910, 1945, np.inf],\n",
    "    [0, 1670, 1830, 1870, 1920, 1945, np.inf],\n",
    "    [0, 1670, 1800, 1870, 1920, 1945, np.inf],\n",
    "    [0, 1670, 1800, 1870, 1920, 1960, np.inf],\n",
    "    [0, 1670, 1830, 1890, 1920, 1945, np.inf],\n",
    "    [0, 1670, 1830, 1890, 1920, 1950, np.inf],\n",
    "    [0, 1670, 1830, 1890, 1910, 1945, np.inf],\n",
    "    [0, 1670, 1830, 1890, 1930, 1975, np.inf],\n",
    "    [0, 1700, 1800, 1870, 1910, 1945, np.inf],\n",
    "    [0, 1700, 1830, 1890, 1910, 1945, np.inf],\n",
    "    [0, 1700, 1830, 1870, 1920, 1945, np.inf],\n",
    "    [0, 1670, 1830, 1870, 1920, 1975, np.inf],\n",
    "    [0, 1670, 1830, 1890, 1920, 1975, np.inf],\n",
    "    [0, 1600, 1700, 1800, 1900, 1950, np.inf],\n",
    "    [0, 1670, 1830, 1920, 1950, 1990, np.inf],\n",
    "    [0, 1700, 1830, 1890, 1910, 1945, np.inf]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec = CountVectorizer(stop_words='english',\n",
    "                        lowercase=True,\n",
    "                        ngram_range=(1, 4),\n",
    "                        strip_accents='unicode')\n",
    "\n",
    "tvec = TfidfVectorizer(stop_words='english',\n",
    "                        ngram_range=(1, 4),\n",
    "                        encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_targets(bin_list, model, vectorizer, df=df):\n",
    "    for b in bin_list:\n",
    "        bins = b\n",
    "        bin_names = range(0, 6)\n",
    "        df['target'] = pd.cut(df['date'], bins, labels=bin_names)\n",
    "        df.groupby('target').count()\n",
    "\n",
    "        #train test split\n",
    "        x = df['text']\n",
    "        y = df['target']\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=.8, random_state=42, shuffle=True, stratify=y)\n",
    "        \n",
    "        #using countvectorizer on the x_train and x_test\n",
    "        train_data = vectorizer.fit_transform(x_train.apply(lambda x: np.str_(x)))\n",
    "        test_data = vectorizer.transform(x_test.apply(lambda x: np.str_(x)))\n",
    "        \n",
    "        \n",
    "        #instantiating, fitting, and scoring the model\n",
    "        model = model\n",
    "        model.fit(train_data, y_train)\n",
    "        score = model.score(test_data, y_test)\n",
    "        print(f' Accuracy of Bin {bins} with {vectorizer}: {score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy of Bin [0, 1670, 1800, 1870, 1910, 1945, inf] with CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 4), preprocessor=None, stop_words='english',\n",
      "                strip_accents='unicode', token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, vocabulary=None): 0.5652173913043478\n",
      " Accuracy of Bin [0, 1670, 1830, 1870, 1910, 1945, inf] with CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 4), preprocessor=None, stop_words='english',\n",
      "                strip_accents='unicode', token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, vocabulary=None): 0.6038647342995169\n",
      " Accuracy of Bin [0, 1670, 1830, 1870, 1920, 1945, inf] with CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 4), preprocessor=None, stop_words='english',\n",
      "                strip_accents='unicode', token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, vocabulary=None): 0.642512077294686\n",
      " Accuracy of Bin [0, 1670, 1800, 1870, 1920, 1945, inf] with CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 4), preprocessor=None, stop_words='english',\n",
      "                strip_accents='unicode', token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, vocabulary=None): 0.6763285024154589\n",
      " Accuracy of Bin [0, 1670, 1800, 1870, 1920, 1960, inf] with CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 4), preprocessor=None, stop_words='english',\n",
      "                strip_accents='unicode', token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, vocabulary=None): 0.6328502415458938\n",
      " Accuracy of Bin [0, 1670, 1830, 1890, 1920, 1945, inf] with CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 4), preprocessor=None, stop_words='english',\n",
      "                strip_accents='unicode', token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, vocabulary=None): 0.6859903381642513\n",
      " Accuracy of Bin [0, 1670, 1830, 1890, 1920, 1950, inf] with CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 4), preprocessor=None, stop_words='english',\n",
      "                strip_accents='unicode', token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, vocabulary=None): 0.6859903381642513\n",
      " Accuracy of Bin [0, 1670, 1830, 1890, 1910, 1945, inf] with CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 4), preprocessor=None, stop_words='english',\n",
      "                strip_accents='unicode', token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, vocabulary=None): 0.6328502415458938\n",
      " Accuracy of Bin [0, 1670, 1830, 1890, 1930, 1975, inf] with CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 4), preprocessor=None, stop_words='english',\n",
      "                strip_accents='unicode', token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, vocabulary=None): 0.5893719806763285\n",
      " Accuracy of Bin [0, 1700, 1800, 1870, 1910, 1945, inf] with CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 4), preprocessor=None, stop_words='english',\n",
      "                strip_accents='unicode', token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, vocabulary=None): 0.6135265700483091\n",
      " Accuracy of Bin [0, 1700, 1830, 1890, 1910, 1945, inf] with CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 4), preprocessor=None, stop_words='english',\n",
      "                strip_accents='unicode', token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, vocabulary=None): 0.6038647342995169\n",
      " Accuracy of Bin [0, 1700, 1830, 1870, 1920, 1945, inf] with CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 4), preprocessor=None, stop_words='english',\n",
      "                strip_accents='unicode', token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, vocabulary=None): 0.6328502415458938\n",
      " Accuracy of Bin [0, 1670, 1830, 1870, 1920, 1975, inf] with CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 4), preprocessor=None, stop_words='english',\n",
      "                strip_accents='unicode', token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, vocabulary=None): 0.5942028985507246\n",
      " Accuracy of Bin [0, 1670, 1830, 1890, 1920, 1975, inf] with CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 4), preprocessor=None, stop_words='english',\n",
      "                strip_accents='unicode', token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, vocabulary=None): 0.6280193236714976\n"
     ]
    }
   ],
   "source": [
    "make_targets(bin_list, model=RandomForestClassifier(), vectorizer=cvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy of Bin [0, 1670, 1800, 1870, 1910, 1945, inf] with CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 5), preprocessor=None, stop_words='english',\n",
      "                strip_accents='unicode', token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, vocabulary=None): 0.5329341317365269\n",
      " Accuracy of Bin [0, 1670, 1830, 1870, 1910, 1945, inf] with CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 5), preprocessor=None, stop_words='english',\n",
      "                strip_accents='unicode', token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, vocabulary=None): 0.5329341317365269\n",
      " Accuracy of Bin [0, 1670, 1830, 1870, 1920, 1945, inf] with CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 5), preprocessor=None, stop_words='english',\n",
      "                strip_accents='unicode', token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, vocabulary=None): 0.5089820359281437\n",
      " Accuracy of Bin [0, 1670, 1800, 1870, 1920, 1945, inf] with CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 5), preprocessor=None, stop_words='english',\n",
      "                strip_accents='unicode', token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, vocabulary=None): 0.49700598802395207\n",
      " Accuracy of Bin [0, 1670, 1800, 1870, 1920, 1960, inf] with CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 5), preprocessor=None, stop_words='english',\n",
      "                strip_accents='unicode', token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, vocabulary=None): 0.46107784431137727\n",
      " Accuracy of Bin [0, 1670, 1830, 1890, 1920, 1945, inf] with CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 5), preprocessor=None, stop_words='english',\n",
      "                strip_accents='unicode', token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, vocabulary=None): 0.5029940119760479\n",
      " Accuracy of Bin [0, 1670, 1830, 1890, 1920, 1950, inf] with CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 5), preprocessor=None, stop_words='english',\n",
      "                strip_accents='unicode', token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, vocabulary=None): 0.5149700598802395\n",
      " Accuracy of Bin [0, 1670, 1830, 1890, 1910, 1945, inf] with CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 5), preprocessor=None, stop_words='english',\n",
      "                strip_accents='unicode', token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, vocabulary=None): 0.49700598802395207\n",
      " Accuracy of Bin [0, 1670, 1830, 1890, 1930, 1975, inf] with CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 5), preprocessor=None, stop_words='english',\n",
      "                strip_accents='unicode', token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, vocabulary=None): 0.49101796407185627\n",
      " Accuracy of Bin [0, 1700, 1800, 1870, 1910, 1945, inf] with CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 5), preprocessor=None, stop_words='english',\n",
      "                strip_accents='unicode', token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, vocabulary=None): 0.5329341317365269\n",
      " Accuracy of Bin [0, 1700, 1830, 1890, 1910, 1945, inf] with CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 5), preprocessor=None, stop_words='english',\n",
      "                strip_accents='unicode', token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, vocabulary=None): 0.49700598802395207\n",
      " Accuracy of Bin [0, 1700, 1830, 1870, 1920, 1945, inf] with CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 5), preprocessor=None, stop_words='english',\n",
      "                strip_accents='unicode', token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, vocabulary=None): 0.5029940119760479\n",
      " Accuracy of Bin [0, 1670, 1830, 1870, 1920, 1975, inf] with CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 5), preprocessor=None, stop_words='english',\n",
      "                strip_accents='unicode', token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, vocabulary=None): 0.49101796407185627\n",
      " Accuracy of Bin [0, 1670, 1830, 1890, 1920, 1975, inf] with CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 5), preprocessor=None, stop_words='english',\n",
      "                strip_accents='unicode', token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, vocabulary=None): 0.46107784431137727\n",
      " Accuracy of Bin [0, 1600, 1700, 1800, 1900, 1950, inf] with CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 5), preprocessor=None, stop_words='english',\n",
      "                strip_accents='unicode', token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, vocabulary=None): 0.5868263473053892\n",
      " Accuracy of Bin [0, 1670, 1830, 1920, 1950, 1990, inf] with CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 5), preprocessor=None, stop_words='english',\n",
      "                strip_accents='unicode', token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, vocabulary=None): 0.5149700598802395\n",
      " Accuracy of Bin [0, 1700, 1830, 1890, 1910, 1945, inf] with CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 5), preprocessor=None, stop_words='english',\n",
      "                strip_accents='unicode', token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, vocabulary=None): 0.49700598802395207\n"
     ]
    }
   ],
   "source": [
    "make_targets(bin_list, model=LogisticRegression(class_weight='balanced'), vectorizer=cvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_targets(bin_list, model=RandomForestClassifier(), vectorizer=tvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_targets(bin_list, model=LogisticRegression(), vectorizer=tvec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binning & Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>info</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>83</td>\n",
       "      <td>83</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>326</td>\n",
       "      <td>326</td>\n",
       "      <td>326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>953</td>\n",
       "      <td>953</td>\n",
       "      <td>953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>97</td>\n",
       "      <td>97</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>181</td>\n",
       "      <td>181</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  info  text\n",
       "target                  \n",
       "0         71    71    71\n",
       "1         83    83    83\n",
       "2        326   326   326\n",
       "3        953   953   953\n",
       "4         97    97    97\n",
       "5        181   181   181"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bins = [0, 1670, 1800, 1870, 1920, 1945, np.inf]\n",
    "names = [0, 1, 2, 3, 4, 5]\n",
    "\n",
    "df['target'] = pd.cut(df['date'], bins, labels=names)\n",
    "\n",
    "\n",
    "df.groupby('target').count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>info</th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1868</td>\n",
       "      <td>['BABBALLADSANDSAVOYSONGS', 'by', 'W.H.GILBERT...</td>\n",
       "      <td>2</td>\n",
       "      <td>['ces handed round on trays.', '', '  Then cur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1900</td>\n",
       "      <td>['BonniePrinceCharlie', 'ATaleofFontenoyandCul...</td>\n",
       "      <td>3</td>\n",
       "      <td>['n fixed in their bed. Of course we have not ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1894</td>\n",
       "      <td>['ArmsandtheMan', 'byGeorgeBernardShaw', 'INTR...</td>\n",
       "      <td>3</td>\n",
       "      <td>['tiful in his physical impotence but strong i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1902</td>\n",
       "      <td>['MARCHINGONNIAGARA', 'ORTHESOLDIERBOYSOFTHEOL...</td>\n",
       "      <td>3</td>\n",
       "      <td>[' of', 'his trading post.\"', '', '\"I\\'m afrai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1914</td>\n",
       "      <td>['NIGHTWATCHES', 'byW.W.Jacobs', 'KEEPINGWATCH...</td>\n",
       "      <td>3</td>\n",
       "      <td>['pper, swelling his chest and looking', \"roun...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   date                                               info target  \\\n",
       "0  1868  ['BABBALLADSANDSAVOYSONGS', 'by', 'W.H.GILBERT...      2   \n",
       "2  1900  ['BonniePrinceCharlie', 'ATaleofFontenoyandCul...      3   \n",
       "3  1894  ['ArmsandtheMan', 'byGeorgeBernardShaw', 'INTR...      3   \n",
       "5  1902  ['MARCHINGONNIAGARA', 'ORTHESOLDIERBOYSOFTHEOL...      3   \n",
       "6  1914  ['NIGHTWATCHES', 'byW.W.Jacobs', 'KEEPINGWATCH...      3   \n",
       "\n",
       "                                                text  \n",
       "0  ['ces handed round on trays.', '', '  Then cur...  \n",
       "2  ['n fixed in their bed. Of course we have not ...  \n",
       "3  ['tiful in his physical impotence but strong i...  \n",
       "5  [' of', 'his trading post.\"', '', '\"I\\'m afrai...  \n",
       "6  ['pper, swelling his chest and looking', \"roun...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "st = df['text'].tolist()\n",
    "\n",
    "\n",
    "def clean_text(raw_text):\n",
    "    raw_text = str(raw_text)\n",
    "    lower_case = raw_text.lower()\n",
    "    retokenizer = RegexpTokenizer(r'[a-z]+')\n",
    "    words = retokenizer.tokenize(lower_case)\n",
    "    \n",
    "    return(lemmatizer.lemmatize(\" \".join(words)))\n",
    "\n",
    "num_excerpts = df['text'].size\n",
    "\n",
    "clean_text_excerpts = []\n",
    "\n",
    "for i in range(0, num_excerpts):\n",
    "     clean_text_excerpts.append( clean_text( st[i] ))\n",
    "\n",
    "\n",
    "df['text'] = clean_text_excerpts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('finaldf_imbalanced_targets.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_class_0, count_class_1, count_class_2, count_class_3, count_class_4, count_class_5 = df.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "count_class_0, count_class_1, count_class_2, count_class_3, count_class_4, count_class_5 = df.target.value_counts()\n",
    "\n",
    "df_class_0 = df[df['target'] == 3] \n",
    "df_class_1 = df[df['target'] == 4] \n",
    "df_class_2 = df[df['target'] == 2]\n",
    "df_class_3 = df[df['target'] == 5]\n",
    "df_class_4 = df[df['target'] == 1]\n",
    "df_class_5 = df[df['target'] == 0]\n",
    "\n",
    "df_class_2 = df_class_2.sample(181, replace=True)\n",
    "df_class_0 = df_class_0.sample(181, replace=True)\n",
    "df_class_1 = df_class_1.sample(181, replace=True)\n",
    "df_class_4 = df_class_4.sample(181, replace=True)\n",
    "df_class_5 = df_class_5.sample(181, replace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "df_text_over = pd.concat([df_class_0, df_class_1, df_class_2, df_class_3, df_class_4, df_class_5])\n",
    "print(df.target.value_counts())\n",
    "print(df_text_over.target.value_counts())\n",
    "\n",
    "df = df_text_over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df['text']\n",
    "y = df['target']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=.8, random_state=42, shuffle=True, stratify=y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec = CountVectorizer(stop_words='english',\n",
    "                        lowercase=True,\n",
    "                        ngram_range=(1, 5),\n",
    "                        strip_accents='unicode')\n",
    "\n",
    "train_data_features = cvec.fit_transform(x_train.apply(lambda x: np.str_(x)))\n",
    "\n",
    "\n",
    "\n",
    "test_data_features = cvec.transform(x_test.apply(lambda x: np.str_(x)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.fit(train_data_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.score(test_data_features, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix \n",
    "\n",
    "y_pred = knn.predict(test_data_features)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))  \n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "# bag = BaggingClassifier(n_estimators=1000, max_samples=0.9, max_features=0.5, bootstrap=True)\n",
    "# bag.fit(train_data_features, y_train)\n",
    "# bag.score(test_data_features, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imbalance Learn Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df['text']\n",
    "y = df['target']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=.8, random_state=42, shuffle=True, stratify=y)\n",
    "\n",
    "cvec = CountVectorizer(stop_words='english',\n",
    "                        lowercase=True,\n",
    "                        ngram_range=(1, 5),\n",
    "                        strip_accents='unicode')\n",
    "\n",
    "x_train = cvec.fit_transform(x_train.apply(lambda x: np.str_(x)))\n",
    "\n",
    "\n",
    "\n",
    "x_test = cvec.transform(x_test.apply(lambda x: np.str_(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rebalance_train_test_logreg(X, y, rebalance_alg, rebalancing_title):\n",
    "\n",
    "    \n",
    "    # Rebalance train data\n",
    "    rebalance = rebalance_alg\n",
    "    x_reb, y_reb = rebalance.fit_sample(x_train, y_train)\n",
    "\n",
    "    # Train a Logistic Regression model on resampled data\n",
    "    logreg = LogisticRegression(solver = 'lbfgs', multi_class = 'auto')\n",
    "    logreg.fit(x_reb, y_reb)\n",
    "\n",
    "    # Generate predictions\n",
    "    y_pred = logreg.predict(x_test)\n",
    "\n",
    "    # Print out metrics\n",
    "    print(f' Accuracy Score: {accuracy_score(y_test, y_pred)}')\n",
    "    print(f' Precision Score: {precision_score(y_test, y_pred, average = None)}')\n",
    "    print(f' Recall Score: {recall_score(y_test, y_pred, average = None)}')\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy Score: 0.38922155688622756\n",
      " Precision Score: [0.3030303  0.30769231 0.44444444 0.28571429 0.4        0.51515152]\n",
      " Recall Score: [0.58823529 0.34782609 0.3902439  0.21052632 0.32258065 0.47222222]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([5, 0, 2, 5, 2, 0, 4, 1, 0, 1, 2, 1, 4, 4, 0, 4, 1, 1, 5, 4, 0, 2,\n",
       "       1, 4, 4, 0, 4, 5, 0, 4, 0, 0, 4, 2, 2, 0, 3, 1, 3, 5, 0, 5, 0, 0,\n",
       "       5, 0, 0, 2, 4, 5, 0, 5, 1, 4, 0, 5, 5, 1, 4, 0, 0, 2, 1, 1, 2, 4,\n",
       "       1, 2, 5, 1, 0, 0, 1, 5, 3, 5, 1, 4, 0, 2, 5, 2, 3, 0, 4, 3, 2, 5,\n",
       "       4, 4, 0, 3, 2, 2, 0, 4, 3, 5, 0, 3, 4, 0, 5, 2, 4, 0, 0, 2, 2, 1,\n",
       "       1, 5, 1, 1, 1, 4, 2, 5, 5, 2, 4, 5, 5, 5, 0, 5, 2, 3, 3, 2, 0, 3,\n",
       "       0, 1, 4, 2, 2, 2, 1, 2, 5, 2, 5, 2, 5, 2, 2, 1, 2, 5, 2, 0, 4, 3,\n",
       "       5, 2, 2, 5, 2, 5, 2, 1, 5, 1, 3, 1, 3])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rebalance_train_test_logreg(x_train, y, SMOTE(), 'SMOTE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rebalance_train_test_logreg(x_train, y, ADASYN(), 'ADASYN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rebalance_train_test_logreg(x_train, y, BorderlineSMOTE(), 'BorderlineSMOTE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy Score: 0.38323353293413176\n",
      " Precision Score: [0.22580645 0.38461538 0.42424242 0.14285714 0.4        0.57575758]\n",
      " Recall Score: [0.41176471 0.43478261 0.34146341 0.10526316 0.38709677 0.52777778]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([5, 0, 2, 5, 2, 0, 4, 1, 4, 5, 4, 2, 4, 4, 0, 4, 1, 1, 5, 4, 0, 2,\n",
       "       1, 2, 4, 0, 2, 4, 0, 4, 0, 0, 4, 2, 3, 0, 3, 0, 3, 5, 5, 5, 1, 5,\n",
       "       5, 1, 0, 2, 3, 5, 2, 5, 5, 0, 3, 5, 0, 1, 4, 4, 0, 2, 2, 1, 3, 4,\n",
       "       1, 2, 5, 1, 5, 0, 1, 0, 1, 5, 3, 1, 0, 2, 5, 2, 2, 0, 4, 3, 1, 4,\n",
       "       4, 4, 1, 3, 2, 1, 0, 4, 2, 5, 4, 2, 4, 0, 0, 4, 3, 0, 0, 4, 4, 1,\n",
       "       1, 0, 4, 0, 1, 4, 2, 3, 5, 2, 4, 5, 0, 5, 5, 5, 2, 3, 3, 4, 5, 2,\n",
       "       0, 1, 1, 2, 2, 4, 5, 1, 5, 1, 5, 2, 5, 2, 2, 0, 2, 0, 2, 0, 4, 2,\n",
       "       5, 2, 1, 5, 2, 5, 2, 1, 5, 0, 3, 1, 5])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rebalance_train_test_logreg(x_train, y, SMOTETomek(), 'SMOTETomek')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rebalance_train_test_logreg(x_train, y, SMOTEENN(), 'SMOTEENN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rebalance_train_test_logreg(x_train, y, RandomUnderSampler(), 'Random')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy Score: 0.4311377245508982\n",
      " Precision Score: [0.31428571 0.5        0.41666667 0.5        0.55       0.5       ]\n",
      " Recall Score: [0.64705882 0.26086957 0.73170732 0.05263158 0.35483871 0.36111111]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 5, 2, 0, 2, 0, 4, 1, 2, 2, 2, 2, 2, 4, 0, 4, 2, 5, 2, 4, 0, 2,\n",
       "       2, 4, 4, 5, 1, 5, 2, 4, 0, 0, 4, 2, 2, 0, 2, 0, 3, 4, 5, 0, 0, 5,\n",
       "       2, 0, 0, 2, 2, 4, 0, 5, 2, 0, 0, 4, 0, 2, 2, 0, 0, 2, 2, 1, 2, 4,\n",
       "       0, 2, 2, 2, 0, 0, 2, 5, 2, 2, 2, 1, 0, 2, 5, 1, 2, 0, 4, 2, 2, 5,\n",
       "       4, 2, 0, 2, 2, 2, 5, 4, 2, 5, 5, 2, 4, 0, 5, 2, 4, 5, 0, 2, 2, 2,\n",
       "       2, 2, 2, 0, 0, 2, 2, 2, 5, 2, 4, 2, 2, 0, 0, 2, 2, 2, 3, 2, 0, 2,\n",
       "       0, 1, 4, 2, 2, 2, 5, 1, 1, 2, 5, 2, 5, 1, 2, 5, 2, 2, 2, 5, 4, 2,\n",
       "       0, 2, 1, 5, 2, 5, 2, 1, 5, 0, 5, 1, 5])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rebalance_train_test_logreg(x_train, y, CondensedNearestNeighbour(), 'CNN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rebalance_train_test_logreg(x_train, y, EditedNearestNeighbours(), 'CNN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rebalance_train_test_logreg(x_train, y, RepeatedEditedNearestNeighbours(), 'CNN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
