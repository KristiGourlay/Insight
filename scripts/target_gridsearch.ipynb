{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import altair as alt\n",
    "import sklearn.metrics as metrics\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "from imblearn.under_sampling import CondensedNearestNeighbour, RandomUnderSampler, EditedNearestNeighbours, RepeatedEditedNearestNeighbours\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE, SVMSMOTE\n",
    "from imblearn.combine import SMOTETomek, SMOTEENN\n",
    "\n",
    "df = pd.read_csv('../data/cleaned/final_df.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    281\n",
       "4    271\n",
       "5    179\n",
       "2    128\n",
       "1    103\n",
       "0     69\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.target.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>info</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1998</td>\n",
       "      <td>The Hours</td>\n",
       "      <td>and here she is herself clarissa not mrs dallo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>1985</td>\n",
       "      <td>Perfume: The Story of a Murderer</td>\n",
       "      <td>when they finally did dare it at first with st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>1999</td>\n",
       "      <td>White Teeth</td>\n",
       "      <td>archie for one watched the mouse he watched it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>2002</td>\n",
       "      <td>Any Human Heart</td>\n",
       "      <td>my personal rollercoaster not so much a roller...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>2008</td>\n",
       "      <td>The Outcast</td>\n",
       "      <td>he didn t think about it he went straight to a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    date                              info  \\\n",
       "49  1998                         The Hours   \n",
       "56  1985  Perfume: The Story of a Murderer   \n",
       "59  1999                       White Teeth   \n",
       "61  2002                   Any Human Heart   \n",
       "63  2008                       The Outcast   \n",
       "\n",
       "                                                 text  \n",
       "49  and here she is herself clarissa not mrs dallo...  \n",
       "56  when they finally did dare it at first with st...  \n",
       "59  archie for one watched the mouse he watched it...  \n",
       "61  my personal rollercoaster not so much a roller...  \n",
       "63  he didn t think about it he went straight to a...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_list = [\n",
    "    [0, 1670, 1800, 1870, 1910, 1945, np.inf],\n",
    "    [0, 1670, 1830, 1870, 1910, 1945, np.inf],\n",
    "    [0, 1670, 1830, 1870, 1920, 1945, np.inf],\n",
    "    [0, 1670, 1800, 1870, 1920, 1945, np.inf],\n",
    "    [0, 1670, 1800, 1870, 1920, 1960, np.inf],\n",
    "    [0, 1670, 1830, 1890, 1920, 1945, np.inf],\n",
    "    [0, 1670, 1830, 1890, 1920, 1950, np.inf],\n",
    "    [0, 1670, 1830, 1890, 1910, 1945, np.inf],\n",
    "    [0, 1670, 1830, 1890, 1930, 1975, np.inf],\n",
    "    [0, 1700, 1800, 1870, 1910, 1945, np.inf],\n",
    "    [0, 1700, 1830, 1890, 1910, 1945, np.inf],\n",
    "    [0, 1700, 1830, 1870, 1920, 1945, np.inf],\n",
    "    [0, 1670, 1830, 1870, 1920, 1975, np.inf],\n",
    "    [0, 1670, 1830, 1890, 1920, 1975, np.inf],\n",
    "    [0, 1600, 1700, 1800, 1900, 1950, np.inf],\n",
    "    [0, 1670, 1830, 1920, 1950, 1990, np.inf],\n",
    "    [0, 1700, 1830, 1890, 1910, 1945, np.inf]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec = CountVectorizer(stop_words='english',\n",
    "                        lowercase=True,\n",
    "                        ngram_range=(1, 4),\n",
    "                        strip_accents='unicode')\n",
    "\n",
    "tvec = TfidfVectorizer(stop_words='english',\n",
    "                        ngram_range=(1, 4),\n",
    "                        encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_targets(bin_list, model, vectorizer, df=df):\n",
    "    for b in bin_list:\n",
    "        bins = b\n",
    "        bin_names = range(0, 6)\n",
    "        df['target'] = pd.cut(df['date'], bins, labels=bin_names)\n",
    "        df.groupby('target').count()\n",
    "\n",
    "        #train test split\n",
    "        x = df['text']\n",
    "        y = df['target']\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=.8, random_state=42, shuffle=True, stratify=y)\n",
    "        \n",
    "        #using countvectorizer on the x_train and x_test\n",
    "        train_data = vectorizer.fit_transform(x_train.apply(lambda x: np.str_(x)))\n",
    "        test_data = vectorizer.transform(x_test.apply(lambda x: np.str_(x)))\n",
    "        \n",
    "        \n",
    "        #instantiating, fitting, and scoring the model\n",
    "        model = model\n",
    "        model.fit(train_data, y_train)\n",
    "        score = model.score(test_data, y_test)\n",
    "        print(f' Accuracy of Bin {bins} with {vectorizer}: {score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "make_targets(bin_list, model=RandomForestClassifier(), vectorizer=cvec) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy of Bin [0, 1670, 1800, 1870, 1910, 1945, inf] with CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 4), preprocessor=None, stop_words='english',\n",
      "                strip_accents='unicode', token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, vocabulary=None): 0.8502415458937198\n",
      " Accuracy of Bin [0, 1670, 1830, 1870, 1910, 1945, inf] with CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 4), preprocessor=None, stop_words='english',\n",
      "                strip_accents='unicode', token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, vocabulary=None): 0.821256038647343\n",
      " Accuracy of Bin [0, 1670, 1830, 1870, 1920, 1945, inf] with CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 4), preprocessor=None, stop_words='english',\n",
      "                strip_accents='unicode', token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, vocabulary=None): 0.8067632850241546\n",
      " Accuracy of Bin [0, 1670, 1800, 1870, 1920, 1945, inf] with CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 4), preprocessor=None, stop_words='english',\n",
      "                strip_accents='unicode', token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, vocabulary=None): 0.8260869565217391\n",
      " Accuracy of Bin [0, 1670, 1800, 1870, 1920, 1960, inf] with CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 4), preprocessor=None, stop_words='english',\n",
      "                strip_accents='unicode', token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, vocabulary=None): 0.8115942028985508\n",
      " Accuracy of Bin [0, 1670, 1830, 1890, 1920, 1945, inf] with CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 4), preprocessor=None, stop_words='english',\n",
      "                strip_accents='unicode', token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, vocabulary=None): 0.8743961352657005\n",
      " Accuracy of Bin [0, 1670, 1830, 1890, 1920, 1950, inf] with CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 4), preprocessor=None, stop_words='english',\n",
      "                strip_accents='unicode', token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, vocabulary=None): 0.8695652173913043\n",
      " Accuracy of Bin [0, 1670, 1830, 1890, 1910, 1945, inf] with CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 4), preprocessor=None, stop_words='english',\n",
      "                strip_accents='unicode', token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, vocabulary=None): 0.8260869565217391\n",
      " Accuracy of Bin [0, 1670, 1830, 1890, 1930, 1975, inf] with CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 4), preprocessor=None, stop_words='english',\n",
      "                strip_accents='unicode', token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, vocabulary=None): 0.8019323671497585\n",
      " Accuracy of Bin [0, 1700, 1800, 1870, 1910, 1945, inf] with CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 4), preprocessor=None, stop_words='english',\n",
      "                strip_accents='unicode', token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, vocabulary=None): 0.8357487922705314\n",
      " Accuracy of Bin [0, 1700, 1830, 1890, 1910, 1945, inf] with CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 4), preprocessor=None, stop_words='english',\n",
      "                strip_accents='unicode', token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, vocabulary=None): 0.8502415458937198\n",
      " Accuracy of Bin [0, 1700, 1830, 1870, 1920, 1945, inf] with CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 4), preprocessor=None, stop_words='english',\n",
      "                strip_accents='unicode', token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, vocabulary=None): 0.8164251207729468\n",
      " Accuracy of Bin [0, 1670, 1830, 1870, 1920, 1975, inf] with CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 4), preprocessor=None, stop_words='english',\n",
      "                strip_accents='unicode', token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, vocabulary=None): 0.7729468599033816\n",
      " Accuracy of Bin [0, 1670, 1830, 1890, 1920, 1975, inf] with CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 4), preprocessor=None, stop_words='english',\n",
      "                strip_accents='unicode', token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, vocabulary=None): 0.8115942028985508\n",
      " Accuracy of Bin [0, 1600, 1700, 1800, 1900, 1950, inf] with CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 4), preprocessor=None, stop_words='english',\n",
      "                strip_accents='unicode', token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, vocabulary=None): 0.9082125603864735\n",
      " Accuracy of Bin [0, 1670, 1830, 1920, 1950, 1990, inf] with CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 4), preprocessor=None, stop_words='english',\n",
      "                strip_accents='unicode', token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, vocabulary=None): 0.821256038647343\n",
      " Accuracy of Bin [0, 1700, 1830, 1890, 1910, 1945, inf] with CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 4), preprocessor=None, stop_words='english',\n",
      "                strip_accents='unicode', token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, vocabulary=None): 0.8502415458937198\n"
     ]
    }
   ],
   "source": [
    "make_targets(bin_list, model=LogisticRegression(class_weight='balanced'), vectorizer=cvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_targets(bin_list, model=RandomForestClassifier(), vectorizer=tvec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy of Bin [0, 1670, 1800, 1870, 1910, 1945, inf] with TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
      "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
      "                min_df=1, ngram_range=(1, 4), norm='l2', preprocessor=None,\n",
      "                smooth_idf=True, stop_words='english', strip_accents=None,\n",
      "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, use_idf=True, vocabulary=None): 0.8260869565217391\n",
      " Accuracy of Bin [0, 1670, 1830, 1870, 1910, 1945, inf] with TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
      "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
      "                min_df=1, ngram_range=(1, 4), norm='l2', preprocessor=None,\n",
      "                smooth_idf=True, stop_words='english', strip_accents=None,\n",
      "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, use_idf=True, vocabulary=None): 0.8309178743961353\n",
      " Accuracy of Bin [0, 1670, 1830, 1870, 1920, 1945, inf] with TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
      "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
      "                min_df=1, ngram_range=(1, 4), norm='l2', preprocessor=None,\n",
      "                smooth_idf=True, stop_words='english', strip_accents=None,\n",
      "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, use_idf=True, vocabulary=None): 0.7922705314009661\n",
      " Accuracy of Bin [0, 1670, 1800, 1870, 1920, 1945, inf] with TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
      "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
      "                min_df=1, ngram_range=(1, 4), norm='l2', preprocessor=None,\n",
      "                smooth_idf=True, stop_words='english', strip_accents=None,\n",
      "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, use_idf=True, vocabulary=None): 0.8115942028985508\n",
      " Accuracy of Bin [0, 1670, 1800, 1870, 1920, 1960, inf] with TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
      "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
      "                min_df=1, ngram_range=(1, 4), norm='l2', preprocessor=None,\n",
      "                smooth_idf=True, stop_words='english', strip_accents=None,\n",
      "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, use_idf=True, vocabulary=None): 0.7536231884057971\n",
      " Accuracy of Bin [0, 1670, 1830, 1890, 1920, 1945, inf] with TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
      "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
      "                min_df=1, ngram_range=(1, 4), norm='l2', preprocessor=None,\n",
      "                smooth_idf=True, stop_words='english', strip_accents=None,\n",
      "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, use_idf=True, vocabulary=None): 0.7729468599033816\n",
      " Accuracy of Bin [0, 1670, 1830, 1890, 1920, 1950, inf] with TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
      "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
      "                min_df=1, ngram_range=(1, 4), norm='l2', preprocessor=None,\n",
      "                smooth_idf=True, stop_words='english', strip_accents=None,\n",
      "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, use_idf=True, vocabulary=None): 0.7681159420289855\n",
      " Accuracy of Bin [0, 1670, 1830, 1890, 1910, 1945, inf] with TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
      "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
      "                min_df=1, ngram_range=(1, 4), norm='l2', preprocessor=None,\n",
      "                smooth_idf=True, stop_words='english', strip_accents=None,\n",
      "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, use_idf=True, vocabulary=None): 0.7246376811594203\n",
      " Accuracy of Bin [0, 1670, 1830, 1890, 1930, 1975, inf] with TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
      "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
      "                min_df=1, ngram_range=(1, 4), norm='l2', preprocessor=None,\n",
      "                smooth_idf=True, stop_words='english', strip_accents=None,\n",
      "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, use_idf=True, vocabulary=None): 0.7439613526570048\n",
      " Accuracy of Bin [0, 1700, 1800, 1870, 1910, 1945, inf] with TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
      "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
      "                min_df=1, ngram_range=(1, 4), norm='l2', preprocessor=None,\n",
      "                smooth_idf=True, stop_words='english', strip_accents=None,\n",
      "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, use_idf=True, vocabulary=None): 0.8067632850241546\n",
      " Accuracy of Bin [0, 1700, 1830, 1890, 1910, 1945, inf] with TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
      "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
      "                min_df=1, ngram_range=(1, 4), norm='l2', preprocessor=None,\n",
      "                smooth_idf=True, stop_words='english', strip_accents=None,\n",
      "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, use_idf=True, vocabulary=None): 0.7294685990338164\n",
      " Accuracy of Bin [0, 1700, 1830, 1870, 1920, 1945, inf] with TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
      "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
      "                min_df=1, ngram_range=(1, 4), norm='l2', preprocessor=None,\n",
      "                smooth_idf=True, stop_words='english', strip_accents=None,\n",
      "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, use_idf=True, vocabulary=None): 0.8019323671497585\n",
      " Accuracy of Bin [0, 1670, 1830, 1870, 1920, 1975, inf] with TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
      "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
      "                min_df=1, ngram_range=(1, 4), norm='l2', preprocessor=None,\n",
      "                smooth_idf=True, stop_words='english', strip_accents=None,\n",
      "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, use_idf=True, vocabulary=None): 0.7342995169082126\n",
      " Accuracy of Bin [0, 1670, 1830, 1890, 1920, 1975, inf] with TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
      "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
      "                min_df=1, ngram_range=(1, 4), norm='l2', preprocessor=None,\n",
      "                smooth_idf=True, stop_words='english', strip_accents=None,\n",
      "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, use_idf=True, vocabulary=None): 0.7246376811594203\n",
      " Accuracy of Bin [0, 1600, 1700, 1800, 1900, 1950, inf] with TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
      "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
      "                min_df=1, ngram_range=(1, 4), norm='l2', preprocessor=None,\n",
      "                smooth_idf=True, stop_words='english', strip_accents=None,\n",
      "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, use_idf=True, vocabulary=None): 0.7101449275362319\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy of Bin [0, 1670, 1830, 1920, 1950, 1990, inf] with TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
      "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
      "                min_df=1, ngram_range=(1, 4), norm='l2', preprocessor=None,\n",
      "                smooth_idf=True, stop_words='english', strip_accents=None,\n",
      "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, use_idf=True, vocabulary=None): 0.6859903381642513\n",
      " Accuracy of Bin [0, 1700, 1830, 1890, 1910, 1945, inf] with TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
      "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
      "                min_df=1, ngram_range=(1, 4), norm='l2', preprocessor=None,\n",
      "                smooth_idf=True, stop_words='english', strip_accents=None,\n",
      "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, use_idf=True, vocabulary=None): 0.7294685990338164\n"
     ]
    }
   ],
   "source": [
    "make_targets(bin_list, model=LogisticRegression(), vectorizer=tvec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binning & Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>info</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>150</td>\n",
       "      <td>150</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>188</td>\n",
       "      <td>188</td>\n",
       "      <td>188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>162</td>\n",
       "      <td>162</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>145</td>\n",
       "      <td>145</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>205</td>\n",
       "      <td>205</td>\n",
       "      <td>205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>181</td>\n",
       "      <td>181</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  info  text\n",
       "target                  \n",
       "0        150   150   150\n",
       "1        188   188   188\n",
       "2        162   162   159\n",
       "3        145   145   145\n",
       "4        205   205   205\n",
       "5        181   181   181"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bins = [0, 1670, 1830, 1870, 1910, 1945, np.inf]\n",
    "names = [0, 1, 2, 3, 4, 5]\n",
    "\n",
    "df['target'] = pd.cut(df['date'], bins, labels=names)\n",
    "\n",
    "\n",
    "df.groupby('target').count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>info</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>1528</td>\n",
       "      <td>The book of the Courtier</td>\n",
       "      <td>then the soul freed from vice purged by studie...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>1569</td>\n",
       "      <td>Planine</td>\n",
       "      <td>his goodly frame the earth seems to me a steri...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>1603</td>\n",
       "      <td>Hamlet</td>\n",
       "      <td>f one lives where all suffer and starve one ac...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>1569</td>\n",
       "      <td>Planine</td>\n",
       "      <td>his goodly frame the earth seems to me a steri...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>1623</td>\n",
       "      <td>macbeth</td>\n",
       "      <td>mine eyes are made the fools o the other sense...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     date                      info  \\\n",
       "242  1528  The book of the Courtier   \n",
       "188  1569                   Planine   \n",
       "189  1603                    Hamlet   \n",
       "188  1569                   Planine   \n",
       "237  1623                   macbeth   \n",
       "\n",
       "                                                  text target  \n",
       "242  then the soul freed from vice purged by studie...      0  \n",
       "188  his goodly frame the earth seems to me a steri...      0  \n",
       "189  f one lives where all suffer and starve one ac...      0  \n",
       "188  his goodly frame the earth seems to me a steri...      0  \n",
       "237  mine eyes are made the fools o the other sense...      0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "st = df['text'].tolist()\n",
    "\n",
    "\n",
    "def clean_text(raw_text):\n",
    "    raw_text = str(raw_text)\n",
    "    lower_case = raw_text.lower()\n",
    "    retokenizer = RegexpTokenizer(r'[a-z]+')\n",
    "    words = retokenizer.tokenize(lower_case)\n",
    "    \n",
    "    return(lemmatizer.lemmatize(\" \".join(words)))\n",
    "\n",
    "num_excerpts = df['text'].size\n",
    "\n",
    "clean_text_excerpts = []\n",
    "\n",
    "for i in range(0, num_excerpts):\n",
    "     clean_text_excerpts.append( clean_text( st[i] ))\n",
    "\n",
    "\n",
    "df['text'] = clean_text_excerpts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../data/cleaned/final_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imbalance Learn Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df['text']\n",
    "y = df['target']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=.8, random_state=42, shuffle=True, stratify=y)\n",
    "\n",
    "cvec = CountVectorizer(stop_words='english',\n",
    "                        lowercase=True,\n",
    "                        ngram_range=(1, 5),\n",
    "                        strip_accents='unicode')\n",
    "\n",
    "x_train = cvec.fit_transform(x_train.apply(lambda x: np.str_(x)))\n",
    "\n",
    "\n",
    "\n",
    "x_test = cvec.transform(x_test.apply(lambda x: np.str_(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rebalance_train_test_logreg(X, y, rebalance_alg, rebalancing_title):\n",
    "\n",
    "    \n",
    "    # Rebalance train data\n",
    "    rebalance = rebalance_alg\n",
    "    x_reb, y_reb = rebalance.fit_sample(x_train, y_train)\n",
    "\n",
    "    # Train a Logistic Regression model on resampled data\n",
    "    logreg = LogisticRegression(solver = 'lbfgs', multi_class = 'auto')\n",
    "    logreg.fit(x_reb, y_reb)\n",
    "\n",
    "    # Generate predictions\n",
    "    y_pred = logreg.predict(x_test)\n",
    "\n",
    "    # Print out metrics\n",
    "    print(f' Accuracy Score: {accuracy_score(y_test, y_pred)}')\n",
    "    print(f' Precision Score: {precision_score(y_test, y_pred, average = None)}')\n",
    "    print(f' Recall Score: {recall_score(y_test, y_pred, average = None)}')\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rebalance_train_test_logreg(x_train, y, SMOTE(), 'SMOTE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rebalance_train_test_logreg(x_train, y, ADASYN(), 'ADASYN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rebalance_train_test_logreg(x_train, y, BorderlineSMOTE(), 'BorderlineSMOTE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rebalance_train_test_logreg(x_train, y, SMOTETomek(), 'SMOTETomek')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rebalance_train_test_logreg(x_train, y, SMOTEENN(), 'SMOTEENN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rebalance_train_test_logreg(x_train, y, RandomUnderSampler(), 'Random')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rebalance_train_test_logreg(x_train, y, CondensedNearestNeighbour(), 'CNN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rebalance_train_test_logreg(x_train, y, EditedNearestNeighbours(), 'CNN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rebalance_train_test_logreg(x_train, y, RepeatedEditedNearestNeighbours(), 'CNN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
