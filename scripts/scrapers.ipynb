
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook has my scrapers for my project. They are separated into the following sections/scrapers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Project Gutenberg](#PG)<a href='#PG'>\n",
    "    \n",
    "* [Stylist Magazine Article](#SMA)<a href='#SMA'>\n",
    "    \n",
    "* [Reddit](#Reddit)<a href='#Reddit'>\n",
    "    \n",
    "* [Date Scrapers](#DS)<a href='#DS'>\n",
    "    \n",
    "* [Online Liberty Fund](#OLF)<a href='#OLF'>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.metrics as metrics\n",
    "import pickle\n",
    "from glob import glob\n",
    "import datetime\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import re\n",
    "\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "import os\n",
    "from time import sleep\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Gutenberg\n",
    "\n",
    "<a id='PG'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Content was scraped using a wget provided by the website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob('../data/raw/Gutenberg/txt/*.txt')\n",
    "\n",
    "book_df = pd.DataFrame(columns=['text', 'info', 'date', 'target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_book_info(files):\n",
    "    '''\n",
    "    Input: list of files\n",
    "    Output: first section of each file which contains book information\n",
    "    '''\n",
    "    book_info = []\n",
    "    for file in files:\n",
    "        open_file = open(file, 'r', encoding= \"ISO-8859-1\")\n",
    "        read_file = open_file.read()[:300]\n",
    "        read_file = read_file.replace(' ', '').split()\n",
    "        book_info.append(read_file)\n",
    "\n",
    "    return book_info\n",
    "\n",
    "\n",
    "book_information = collect_book_info(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_book_text(files):\n",
    "    '''\n",
    "    Input: list of files\n",
    "    Output: portion of each file that contains excerpt\n",
    "    '''\n",
    "    book_text = []\n",
    "    for file in files:\n",
    "        open_file = open(file, 'r', encoding= \"ISO-8859-1\")\n",
    "        read_file = open_file.read()[1500:2300].splitlines()\n",
    "        book_text.append(read_file)\n",
    "\n",
    "    return book_text\n",
    "\n",
    "book_text = collect_book_text(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding information to dataframe\n",
    "book_df['info'] = book_information\n",
    "book_df['text'] = book_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_half_words(raw_text):\n",
    "    '''\n",
    "    Input: column in df that has excerpts extracted from files\n",
    "    Output: excerpts with first and last words dropped to eliminate half words\n",
    "    '''\n",
    "    words = str(raw_text).split()\n",
    "    result = words[1:-2]\n",
    "    return result\n",
    "\n",
    "book_df['text'] = book_df['text'].apply(delete_half_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting Date Information\n",
    "def create_date(book_info):\n",
    "    date_info = []\n",
    "    for info in book_information:\n",
    "        info = str(info)\n",
    "        info = re.findall(r'(\\d{4})', info)\n",
    "        date_info.append(info)\n",
    "\n",
    "    return date_info\n",
    "\n",
    "book_dates = create_date(book_information)\n",
    "book_df['date'] = book_dates\n",
    "\n",
    "book_df = book_df.replace('[]', np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_df['date'] = book_df['date'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_df['date'] = book_df['date'].str.strip('[]')\n",
    "book_df['date'] = book_df['date'].str.strip(\"''\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mask = (book_df['date'].str.len() > 4)\n",
    "\n",
    "len(mask)\n",
    "book_df[mask] = book_df.loc[mask].date = 'NaN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('../data/processed/pg.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stylist Magazine Article\n",
    "\n",
    "<a id='SMA'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://stylist.co.uk/books/the-best-100-closing-lines-from-books/123681'\n",
    "res = requests.get(url)\n",
    "soup = bs(res.content, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books = []\n",
    "for link in soup.find_all('h3', {'class': 'css-r1u8am'}):\n",
    "    ta_dict = {}\n",
    "    ta_dict['title'] = link.text\n",
    "    books.append(ta_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes = []\n",
    "for link in soup.find_all('div', {'class': 'css-dbbd7o'}):\n",
    "    tr_dict = {}\n",
    "    tr_dict['quote'] = link.text\n",
    "    quotes.append(tr_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_info = pd.DataFrame(books, quotes)\n",
    "book_info = book_info.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_quotes = book_info['index'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_quote(list_of_quotes):\n",
    "    '''\n",
    "    Input: List of dictionaries\n",
    "    Output: Unpacked dictionary, quotes (values) saved to list'''\n",
    "    quotes = []\n",
    "    num = len(list_of_quotes)\n",
    "    for i in range(0, num):\n",
    "        new_quote = list_of_quotes[i]['quote'].splitlines()[0]\n",
    "        quotes.append(new_quote)\n",
    "    \n",
    "    return quotes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "book_info['index'] = extract_quote(list_of_quotes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_info = book_info.rename(columns={'index': 'text'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = book_info[book_info['text'].map(len) > 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target'] = 'NaN'\n",
    "df['date'] = 'NaN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('../data/processed/quote_scrape.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit\n",
    "\n",
    "<a id='Reddit'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = 'https://www.reddit.com/r/books/comments/1mqfzt/what_is_the_most_powerful_chapter_paragraph_or/.json'\n",
    "# url = 'https://thoughtcatalog.com/charlie-shaw/2013/09/34-profound-excerpts-from-classic-literature-that-will-change-your-day/.json'\n",
    "# url = 'https://www.reddit.com/r/books/comments/35wv34/whats_the_most_beautiful_paragraph_or_sentence/.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'User-Agent': 'My User Agent 1.0'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_page(url, after=''):\n",
    "    params = {'after': after}\n",
    "    response = requests.get(url, headers=headers, params=params)\n",
    "    return response.json()[1]['data']['children']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_post(post):\n",
    "    keep = ['subreddit', 'title', 'body', 'name'] \n",
    "    return {k:v for k, v in post['data'].items() if k in keep}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_page(page):\n",
    "    after = ''\n",
    "    posts = []\n",
    "    for post in page:\n",
    "        post = parse_post(post)\n",
    "        after = post['name']\n",
    "        posts.append(post)\n",
    "    return posts, after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_subreddit(subreddit, pages=4):\n",
    "    url = subreddit\n",
    "    after = ''\n",
    "    all_posts = []\n",
    "    for i in range(pages):\n",
    "        print(f'Fetching Page {i + 1}')\n",
    "        page = fetch_page(url, after)\n",
    "        posts, after = parse_page(page)\n",
    "        all_posts.extend(posts)\n",
    "        time.sleep(5)\n",
    "    return all_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = fetch_subreddit(url, pages=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df = pd.DataFrame(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df = reddit_df.dropna()\n",
    "reddit_df = reddit_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_posts(posts):\n",
    "    new_posts = []\n",
    "    for post in posts:\n",
    "        post = post.replace('\\n', '')\n",
    "        new_posts.append(post)\n",
    "        \n",
    "    return new_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = reddit_df['body'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df['body'] = clean_posts(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_column(df, col):\n",
    "    '''\n",
    "    Input: dataframe and specified column\n",
    "    Output: column split into two columns by the last existence of a dash (how a majority of redditor's split excerpt and book)'''\n",
    "    new = df[col].str.rsplit('-', n=1, expand=True)\n",
    "    df['text'] = new[0]\n",
    "    df['info'] = new[1]\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = split_column(reddit_df, 'body')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('reddit_data.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.append(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('../data/processed/reddit_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('../data/processed/reddit_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Date Scraping (Selenium) \n",
    "\n",
    "<a id='DS'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/processed/reddit_data.csv', index_col=0)\n",
    "# browser = webdriver.Firefox()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(subset=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('../data/processed/quote_scrape.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['name', 'subreddit', 'body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.drop(columns=['target', 'date'])\n",
    "df2 = df2.rename(columns={'title': 'info'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.append(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(subset=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_list = df['info'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_queries = []\n",
    "for item in title_list:\n",
    "    search_queries.append(f'when was {item} published?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scrape for Google.com\n",
    "\n",
    "dates  = []\n",
    "browser = webdriver.Firefox()\n",
    "for p in search_queries:\n",
    "    browser.get('http://www.google.com')\n",
    "    search = browser.find_element_by_name('q')\n",
    "    search.send_keys(f'\"{p}\"')\n",
    "    search.send_keys(Keys.RETURN) # hit return after you enter search text\n",
    "    time.sleep(60) #sleep for 20 seconds\n",
    "    \n",
    "    try:\n",
    "        result = browser.find_element_by_xpath('.//div[@class=\"Z0LcW\"]') \n",
    "        result = result.get_attribute('innerHTML') #if date number exists, append to list\n",
    "    except:\n",
    "        result = 'NaN' #if date number does not exist, append \"NaN\"\n",
    "\n",
    "    dates.append(result)\n",
    "browser.quit()\n",
    "\n",
    "# Google's selenium scraper dealt with recaptchas more often so the sleeps had to be longer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_years(text):\n",
    "    new_list_dates = []\n",
    "    for t in text:\n",
    "        date = re.findall(r'(\\d{4})', t)\n",
    "        new_list_dates.append(date)\n",
    "                          \n",
    "    return new_list_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>info</th>\n",
       "      <th>google_dates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>\"Archie, for one, watched the mouse. He watche...</td>\n",
       "      <td>White Teeth</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>\"Might I trouble you then to be ready in half ...</td>\n",
       "      <td>The Hound of the Baskervilles</td>\n",
       "      <td>[1902]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>\"My personal rollercoaster. Not so much a roll...</td>\n",
       "      <td>Any Human Heart</td>\n",
       "      <td>[2002]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>\"The offing was barred by a black bank of clou...</td>\n",
       "      <td>Heart of Darkness</td>\n",
       "      <td>[1902]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>\"He didn't think about it, he went straight to...</td>\n",
       "      <td>The Outcast</td>\n",
       "      <td>[1918]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  \\\n",
       "133  \"Archie, for one, watched the mouse. He watche...   \n",
       "134  \"Might I trouble you then to be ready in half ...   \n",
       "135  \"My personal rollercoaster. Not so much a roll...   \n",
       "136  \"The offing was barred by a black bank of clou...   \n",
       "137  \"He didn't think about it, he went straight to...   \n",
       "\n",
       "                              info google_dates  \n",
       "133                    White Teeth           []  \n",
       "134  The Hound of the Baskervilles       [1902]  \n",
       "135                Any Human Heart       [2002]  \n",
       "136              Heart of Darkness       [1902]  \n",
       "137                    The Outcast       [1918]  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['google_dates'] = extract_years(dates)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape for DuckDuckGo.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_sq = []\n",
    "for item in title_list:\n",
    "    dd_sq.append(f'what date was {item} published?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_date = []\n",
    "\n",
    "browser = webdriver.Firefox()\n",
    "for p in dd_sq:\n",
    "    browser.get('http://www.duckduckgo.com')\n",
    "    search = browser.find_element_by_xpath(\".//input[@id='search_form_input_homepage']\")\n",
    "    search.send_keys(f'{p}')\n",
    "    search.send_keys(Keys.RETURN) # hit return after you enter search text\n",
    "    time.sleep(20) # sleep for 5 seconds \n",
    "    \n",
    "    try:\n",
    "        result = browser.find_element_by_xpath('.//span[@class=\"js-about-item-abstr\"]')\n",
    "        result = result.get_attribute('innerHTML')\n",
    "    except:\n",
    "        result = 'NaN'\n",
    "\n",
    "    dd_date.append(result)\n",
    "\n",
    "    \n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_date\n",
    "df['duck_dates'] = extract_years(dd_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scrape for Ask.com\n",
    "\n",
    "ask_dates = []\n",
    "\n",
    "browser = webdriver.Firefox()\n",
    "for p in dd_sq:\n",
    "    browser.get('http://www.ask.com')\n",
    "    search = browser.find_element_by_name('q')\n",
    "    search.send_keys(f'{p}')\n",
    "    search.send_keys(Keys.RETURN) # hit return after you enter search text\n",
    "    time.sleep(10) # sleep for 5 seconds so you can see the results\n",
    "    \n",
    "    try:\n",
    "        result = browser.find_element_by_xpath('.//p[@class=\"PartialSearchResults-item-abstract\"]')\n",
    "        result = result.get_attribute('innerHTML')\n",
    "    except:\n",
    "        result = 'NaN'\n",
    "\n",
    "    ask_dates.append(result)\n",
    "browser.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ask_dates'] = extract_years(ask_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('../data/processed/reddit_df_with_scraped_dates.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>info</th>\n",
       "      <th>google_dates</th>\n",
       "      <th>ask_dates</th>\n",
       "      <th>duck_dates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'If people bring so much courage to this world...</td>\n",
       "      <td>A Farewell to Arms</td>\n",
       "      <td>[1929]</td>\n",
       "      <td>[1929]</td>\n",
       "      <td>['1929']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"I looked at the stars, and considered how awf...</td>\n",
       "      <td>Dickens, Great Expectations</td>\n",
       "      <td>[1861]</td>\n",
       "      <td>[1860, 1861]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"As the days went by, the evolution of like in...</td>\n",
       "      <td>Jack London</td>\n",
       "      <td>[1916]</td>\n",
       "      <td>[2019]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"Where else? I belong to a lost generation and...</td>\n",
       "      <td>Umberto Eco, Foucault's Pendulum</td>\n",
       "      <td>[1988]</td>\n",
       "      <td>[1988, 1988, 1989]</td>\n",
       "      <td>['1988']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>'Have you ever been in love? Horrible isn't it...</td>\n",
       "      <td>The Sandman, Neil Gaiman</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  'If people bring so much courage to this world...   \n",
       "1  \"I looked at the stars, and considered how awf...   \n",
       "2  \"As the days went by, the evolution of like in...   \n",
       "3  \"Where else? I belong to a lost generation and...   \n",
       "4  'Have you ever been in love? Horrible isn't it...   \n",
       "\n",
       "                                info google_dates           ask_dates  \\\n",
       "0                 A Farewell to Arms       [1929]              [1929]   \n",
       "1        Dickens, Great Expectations       [1861]        [1860, 1861]   \n",
       "2                        Jack London       [1916]              [2019]   \n",
       "3   Umberto Eco, Foucault's Pendulum       [1988]  [1988, 1988, 1989]   \n",
       "4           The Sandman, Neil Gaiman           []                  []   \n",
       "\n",
       "  duck_dates  \n",
       "0   ['1929']  \n",
       "1         []  \n",
       "2         []  \n",
       "3   ['1988']  \n",
       "4         []  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Online Liberty Fund (NonFiction)\n",
    "\n",
    "<a id='OLF'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://oll.libertyfund.org/groups/44'\n",
    "res = requests.get(url)\n",
    "\n",
    "soup = bs(res.content, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scrapinghttps://oll.libertyfund.org/titles/titles/molesworth-an-account-of-denmark-with-francogallia-and-some-considerations-for-the-promoting-of-agriculture-and-employing-the-poor\n",
      "scrapinghttps://oll.libertyfund.org/titles/titles/spooner-address-of-the-free-constitutionalists-to-the-people-of-the-united-states-1860\n",
      "scrapinghttps://oll.libertyfund.org/titles/titles/condorcet-on-the-admission-of-women-to-the-rights-of-citizenship\n",
      "scrapinghttps://oll.libertyfund.org/titles/titles/bryce-the-american-commonwealth-2-vols\n",
      "scrapinghttps://oll.libertyfund.org/titles/titles/bryce-the-american-commonwealth-vol-1\n",
      "scrapinghttps://oll.libertyfund.org/titles/titles/bryce-the-american-commonwealth-vol-2\n",
      "scrapinghttps://oll.libertyfund.org/titles/titles/hyneman-american-political-writing-during-the-founding-era-1760-1805-2-vols\n",
      "scrapinghttps://oll.libertyfund.org/titles/titles/hyneman-american-political-writing-during-the-founding-era-1760-1805-vol-1\n",
      "scrapinghttps://oll.libertyfund.org/titles/titles/hyneman-american-political-writing-during-the-founding-era-1760-1805-vol-2\n",
      "scrapinghttps://oll.libertyfund.org/titles/titles/frohnen-the-american-republic-primary-sources\n",
      "scrapinghttps://oll.libertyfund.org/titles/titles/spencer-an-autobiography-2-vols-1904\n",
      "scrapinghttps://oll.libertyfund.org/titles/titles/spencer-an-autobiography-vol-1\n"
     ]
    }
   ],
   "source": [
    "slugs = []\n",
    "links_for_books = []\n",
    "list_of_sites = []\n",
    "text_info = []\n",
    "\n",
    "for link in soup.find_all('li'):\n",
    "    l = link.find('a')\n",
    "    slugs.append(l)\n",
    "\n",
    "for row in slugs:\n",
    "    r = str(row)\n",
    "    link = re.findall(r'\"(.*?)\"', r)\n",
    "    links_for_books.append(link)\n",
    "\n",
    "for slug in links_for_books:\n",
    "    slugname = str(slug[0])\n",
    "    link = f'https://oll.libertyfund.org/titles{slugname}'\n",
    "    print(f'scraping{link}')\n",
    "       \n",
    "    res = requests.get(link)\n",
    "    soup = bs(res.content, 'lxml')\n",
    "    words = soup.get_text()[1000:2000]\n",
    "    text = words.strip()\n",
    "    text = text.replace('\\n', '')\n",
    "    text_info.append(text)\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "olf_df = pd.DataFrame(text_info)\n",
    "# olf_df.to_csv('../data/processed/olf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
