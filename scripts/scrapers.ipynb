{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.metrics as metrics\n",
    "import pickle\n",
    "from glob import glob\n",
    "import datetime\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import re\n",
    "\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "import os\n",
    "from time import sleep\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Content was scraped using a wget provided by the website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob('../data/raw/Gutenberg/txt/*.txt')\n",
    "\n",
    "book_df = pd.DataFrame(columns=['text', 'info', 'date', 'target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_book_info(files):\n",
    "    '''\n",
    "    Input: list of files\n",
    "    Output: first section of each file which contains book information\n",
    "    '''\n",
    "    book_info = []\n",
    "    for file in files:\n",
    "        open_file = open(file, 'r', encoding= \"ISO-8859-1\")\n",
    "        read_file = open_file.read()[:300]\n",
    "        read_file = read_file.replace(' ', '').split()\n",
    "        book_info.append(read_file)\n",
    "\n",
    "    return book_info\n",
    "\n",
    "\n",
    "book_information = collect_book_info(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_book_text(files):\n",
    "    '''\n",
    "    Input: list of files\n",
    "    Output: portion of each file that contains excerpt\n",
    "    '''\n",
    "    book_text = []\n",
    "    for file in files:\n",
    "        open_file = open(file, 'r', encoding= \"ISO-8859-1\")\n",
    "        read_file = open_file.read()[1500:2300].splitlines()\n",
    "        book_text.append(read_file)\n",
    "\n",
    "    return book_text\n",
    "\n",
    "book_text = collect_book_text(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding information to dataframe\n",
    "book_df['info'] = book_information\n",
    "book_df['text'] = book_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_half_words(raw_text):\n",
    "    '''\n",
    "    Input: column in df that has excerpts extracted from files\n",
    "    Output: excerpts with first and last words dropped to elimiate half words\n",
    "    '''\n",
    "    words = str(raw_text).split()\n",
    "    result = words[1:-2]\n",
    "    return result\n",
    "\n",
    "book_df['text'] = book_df['text'].apply(delete_half_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting Date Information\n",
    "def create_date(book_info):\n",
    "    date_info = []\n",
    "    for info in book_information:\n",
    "        info = str(info)\n",
    "        info = re.findall(r'(\\d{4})', info)\n",
    "        date_info.append(info)\n",
    "\n",
    "    return date_info\n",
    "\n",
    "book_dates = create_date(book_information)\n",
    "book_df['date'] = book_dates\n",
    "\n",
    "book_df = book_df.replace('[]', np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_df['date'] = book_df['date'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_df['date'] = book_df['date'].str.strip('[]')\n",
    "book_df['date'] = book_df['date'].str.strip(\"''\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kristigourlay/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py:5096: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self[name] = value\n"
     ]
    }
   ],
   "source": [
    "mask = (book_df['date'].str.len() > 4)\n",
    "\n",
    "len(mask)\n",
    "book_df[mask] = book_df.loc[mask].date = 'NaN'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stylist Magazine Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import re\n",
    "\n",
    "\n",
    "url = 'http://stylist.co.uk/books/the-best-100-closing-lines-from-books/123681'\n",
    "res = requests.get(url)\n",
    "soup = bs(res.content, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "books = []\n",
    "for link in soup.find_all('h3', {'class': 'css-r1u8am'}):\n",
    "    ta_dict = {}\n",
    "    ta_dict['title'] = link.text\n",
    "    books.append(ta_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes = []\n",
    "for link in soup.find_all('div', {'class': 'css-dbbd7o'}):\n",
    "    tr_dict = {}\n",
    "    tr_dict['quote'] = link.text\n",
    "    quotes.append(tr_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_info = pd.DataFrame(books, quotes)\n",
    "book_info = book_info.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_quotes = book_info['index'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_quote(list_of_quotes):\n",
    "    '''\n",
    "    Input: List of dictionaries\n",
    "    Output: Unpacked dictionary, quotes (values) saved to list'''\n",
    "    quotes = []\n",
    "    num = len(list_of_quotes)\n",
    "    for i in range(0, num):\n",
    "        new_quote = list_of_quotes[i]['quote'].splitlines()[0]\n",
    "        quotes.append(new_quote)\n",
    "    \n",
    "    return quotes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "book_info['index'] = extract_quote(list_of_quotes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_info = book_info.rename(columns={'index': 'text'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = book_info[book_info['text'].map(len) > 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target'] = 'NaN'\n",
    "df['date'] = 'NaN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('quote_scrape.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = 'https://www.reddit.com/r/books/comments/1mqfzt/what_is_the_most_powerful_chapter_paragraph_or/.json'\n",
    "# url = 'https://thoughtcatalog.com/charlie-shaw/2013/09/34-profound-excerpts-from-classic-literature-that-will-change-your-day/.json'\n",
    "url = 'https://www.reddit.com/r/books/comments/35wv34/whats_the_most_beautiful_paragraph_or_sentence/.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'User-Agent': 'My User Agent 1.0'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_page(url, after=''):\n",
    "    params = {'after': after}\n",
    "    response = requests.get(url, headers=headers, params=params)\n",
    "    return response.json()[1]['data']['children']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_post(post):\n",
    "    keep = ['subreddit', 'title', 'body', 'name'] \n",
    "    return {k:v for k, v in post['data'].items() if k in keep}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_page(page):\n",
    "    after = ''\n",
    "    posts = []\n",
    "    for post in page:\n",
    "        post = parse_post(post)\n",
    "        after = post['name']\n",
    "        posts.append(post)\n",
    "    return posts, after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_subreddit(subreddit, pages=4):\n",
    "    url = subreddit\n",
    "    after = ''\n",
    "    all_posts = []\n",
    "    for i in range(pages):\n",
    "        print(f'Fetching Page {i + 1}')\n",
    "        page = fetch_page(url, after)\n",
    "        posts, after = parse_page(page)\n",
    "        all_posts.extend(posts)\n",
    "        time.sleep(5)\n",
    "    return all_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = fetch_subreddit(url, pages=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df = pd.DataFrame(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df = reddit_df.dropna()\n",
    "reddit_df = reddit_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_posts(posts):\n",
    "    new_posts = []\n",
    "    for post in posts:\n",
    "        post = post.replace('\\n', '')\n",
    "        new_posts.append(post)\n",
    "        \n",
    "    return new_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = reddit_df['body'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df['body'] = clean_posts(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_column(df, col):\n",
    "    '''\n",
    "    Input: dataframe and specified column\n",
    "    Output: column split into two columns by the last existence of a dash (how a majority of redditor's split excerpt and book)'''\n",
    "    new = df[col].str.rsplit('-', n=1, expand=True)\n",
    "    df['text'] = new[0]\n",
    "    df['info'] = new[1]\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = split_column(reddit_df, 'body')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('reddit_data.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.append(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('reddit_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('reddit_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Date Scraping (Selenium) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/processed/reddit_data.csv', index_col=0)\n",
    "# browser = webdriver.Firefox()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(subset=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('../data/processed/quote_scrape.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['name', 'subreddit', 'body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.drop(columns=['target', 'date'])\n",
    "df2 = df2.rename(columns={'title': 'info'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.append(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(subset=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_list = df['info'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_queries = []\n",
    "for item in title_list:\n",
    "    search_queries.append(f'when was {item} published?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scrape for Google.com\n",
    "\n",
    "dates  = []\n",
    "browser = webdriver.Firefox()\n",
    "for p in search_queries:\n",
    "    browser.get('http://www.google.com')\n",
    "    search = browser.find_element_by_name('q')\n",
    "    search.send_keys(f'\"{p}\"')\n",
    "    search.send_keys(Keys.RETURN) # hit return after you enter search text\n",
    "    time.sleep(60) #sleep for 20 seconds\n",
    "    \n",
    "    try:\n",
    "        result = browser.find_element_by_xpath('.//div[@class=\"Z0LcW\"]') \n",
    "        result = result.get_attribute('innerHTML') #if date number exists, append to list\n",
    "    except:\n",
    "        result = 'NaN' #if date number does not exist, append \"NaN\"\n",
    "\n",
    "    dates.append(result)\n",
    "browser.quit()\n",
    "\n",
    "# Googles selenium scraper dealt with recaptchas more often so the sleeps had to be longer, and it would need to be \n",
    "#stopped and restarted at the index spot where it left off. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_years(text):\n",
    "    new_list_dates = []\n",
    "    for t in text:\n",
    "        date = re.findall(r'(\\d{4})', t)\n",
    "        new_list_dates.append(date)\n",
    "                          \n",
    "    return new_list_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['google_dates'] = extract_years(dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape for DuckDuckGo.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_sq = []\n",
    "for item in title_list:\n",
    "    dd_sq.append(f'what date was {item} published?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_date = []\n",
    "\n",
    "browser = webdriver.Firefox()\n",
    "for p in dd_sq:\n",
    "    browser.get('http://www.duckduckgo.com')\n",
    "    search = browser.find_element_by_xpath(\".//input[@id='search_form_input_homepage']\")\n",
    "    search.send_keys(f'{p}')\n",
    "    search.send_keys(Keys.RETURN) # hit return after you enter search text\n",
    "    time.sleep(10) # sleep for 5 seconds \n",
    "    \n",
    "    try:\n",
    "        result = browser.find_element_by_xpath('.//span[@class=\"js-about-item-abstr\"]')\n",
    "        result = result.get_attribute('innerHTML')\n",
    "    except:\n",
    "        result = 'NaN'\n",
    "\n",
    "    dd_date.append(result)\n",
    "\n",
    "    \n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_date\n",
    "df['duck_dates'] = extract_years(dd_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scrape for Ask.com\n",
    "\n",
    "ask_dates = []\n",
    "\n",
    "browser = webdriver.Firefox()\n",
    "for p in dd_sq:\n",
    "    browser.get('http://www.ask.com')\n",
    "    search = browser.find_element_by_name('q')\n",
    "    search.send_keys(f'{p}')\n",
    "    search.send_keys(Keys.RETURN) # hit return after you enter search text\n",
    "    time.sleep(10) # sleep for 5 seconds so you can see the results\n",
    "    \n",
    "    try:\n",
    "        result = browser.find_element_by_xpath('.//p[@class=\"PartialSearchResults-item-abstract\"]')\n",
    "        result = result.get_attribute('innerHTML')\n",
    "    except:\n",
    "        result = 'NaN'\n",
    "\n",
    "    ask_dates.append(result)\n",
    "browser.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ask_dates'] = extract_years(ask_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../data/processed/reddit_df_with_scraped_dates.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Online Liberty Fund (NonFiction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://oll.libertyfund.org/groups/44'\n",
    "res = requests.get(url)\n",
    "\n",
    "soup = bs(res.content, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slugs = []\n",
    "links_for_books = []\n",
    "list_of_sites = []\n",
    "text_info = []\n",
    "\n",
    "for link in soup.find_all('li'):\n",
    "    l = link.find('a')\n",
    "    slugs.append(l)\n",
    "\n",
    "for row in slugs:\n",
    "    r = str(row)\n",
    "    link = re.findall(r'\"(.*?)\"', r)\n",
    "    links_for_books.append(link)\n",
    "\n",
    "for slug in links_for_books:\n",
    "    slugname = str(slug[0])\n",
    "    link = f'https://oll.libertyfund.org/titles{slugname}'\n",
    "    print(f'scraping{link}')\n",
    "       \n",
    "    res = requests.get(link)\n",
    "    soup = bs(res.content, 'lxml')\n",
    "    words = soup.get_text()[1000:2000]\n",
    "    text = words.strip()\n",
    "    text = text.replace('\\n', '')\n",
    "    text_info.append(text)\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "olf_df = pd.DataFrame(text_info)\n",
    "# olf_df.to_csv('olf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
